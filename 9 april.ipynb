{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45334c15-533f-4ab5-8c7f-92f22828d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00adc92f-910a-4755-b88b-987382225a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "To predict the class of a new instance with features X1=3 and X2=4 using Naive Bayes, we need to calculate the posterior probabilities for each class and choose the one with the highest probability.\n",
    "\n",
    "The Naive Bayes algorithm assumes that the features are conditionally independent given the class label, so we can calculate the posterior probability for each class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8cc22f-55ae-435f-ba37-af10800a9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "P(A | X1=3, X2=4) = P(X1=3 | A) * P(X2=4 | A) * P(A)\n",
    "                = 4/10 * 3/10 * 1/2 \n",
    "\n",
    "                = 0.06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2e28a-6cc0-42ef-ba51-6ac37449198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "P(B | X1=3, X2=4) = P(X1=3 | B) * P(X2=4 | B) * P(B)\n",
    "                = 1/7 * 3/7 * 1/2 \n",
    "\n",
    "                = 0.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45de77-bbe9-445e-92c8-8b1dabe8e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Since P(A | X1=3, X2=4) > P(B | X1=3, X2=4), the Naive Bayes classifier would predict that \n",
    "the new instance belongs to class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923e4be4-62b5-49c4-8432-ca9f725285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514960a-aa37-424e-b0c5-7b7af08594a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It states that the probability of an event A occurring given that event B has occurred is proportional to the probability of event B occurring given that event A has occurred, multiplied by the prior probability of event A occurring, and divided by the prior probability of event B occurring. In mathematical notation, Bayes' theorem can be expressed as:\n",
    "\n",
    "P(A | B) = P(B | A) * P(A) / P(B)\n",
    "\n",
    "where P(A | B) is the probability of event A occurring given that event B has occurred, P(B | A) is the probability of event B occurring given that event A has occurred,\n",
    "P(A) is the prior probability of event A occurring, and P(B) is the prior probability of event B occurring.\n",
    "\n",
    "Bayes' theorem has many applications in various fields, including statistics,\n",
    "machine learning, and artificial intelligence, where it is often used to update beliefs or make predictions based on new evidence or data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3f45cc-bc25-48f0-ba27-6eb18ee86a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45979a-6004-4ab5-a16b-2a8fd5a76b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It can be expressed mathematically as:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the conditional probability of event A occurring given that event B has occurred.\n",
    "P(B|A) is the conditional probability of event B occurring given that event A has occurred.\n",
    "P(A) is the prior probability of event A occurring.\n",
    "P(B) is the prior probability of event B occurring.\n",
    "Bayes' theorem is used to update beliefs or make predictions based on new evidence or data. By using conditional probabilities and prior probabilities, it provides a way to reason about uncertain events and make decisions based on incomplete information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474090ba-3933-4bc7-8e6b-e25d579cc2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975c265-278e-4d56-a4cb-ad1d44a0c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is widely used in various fields, including statistics, machine learning, and artificial intelligence, to make predictions or update beliefs based on new evidence or data. Here are a few examples of how Bayes' theorem is used in practice:\n",
    "\n",
    "Medical diagnosis: Bayes' theorem is used to calculate the probability of a patient having a disease based on the results of medical tests. For example, if a test has a false positive rate of 5%, and the prevalence of the disease in the population is 0.1%, Bayes' theorem can be used to calculate the probability of a patient actually having the disease given a positive test result.\n",
    "\n",
    "Spam filtering: Bayes' theorem is used to classify emails as spam or non-spam based on the presence or absence of certain words or phrases. A spam filter can be trained using a set of known spam and non-spam emails, and Bayes' theorem can be used to calculate the probability of an email being spam given the words or phrases it contains.\n",
    "\n",
    "Image recognition: Bayes' theorem is used to classify images into different categories based on their features or characteristics. For example, a machine learning algorithm can be trained to recognize faces by analyzing the features of different faces, and Bayes' theorem can be used to calculate the probability of a new image being a face given its features.\n",
    "\n",
    "Fraud detection: Bayes' theorem is used to detect fraudulent activities in financial transactions by analyzing patterns of behavior and identifying anomalies. For example, if a customer suddenly makes a large purchase in a foreign country, Bayes' theorem can be used to calculate the probability of the transaction being fraudulent based on past behavior and transaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5a4dbe-eeea-4073-8b66-d5b41bbeff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2580dd57-a20c-44de-866f-323e7c3db5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is a mathematical formula that describes the relationship between conditional probabilities. Specifically, it provides a way to calculate the probability of an event A given the occurrence of an event B, in terms of the probability of event B given event A and the probabilities of events A and B separately.\n",
    "\n",
    "In other words, Bayes' theorem allows us to update our beliefs about the probability of an event based on new information. It is often used in situations where we have some prior knowledge or belief about the probability of an event, and we want to revise that belief based on new evidence.\n",
    "\n",
    "Conditional probability, on the other hand, is a basic concept in probability theory that describes the probability of an event occurring given that another event has already occurred. It is often written as P(A|B), where A and B are events, and represents the probability of event A given that event B has occurred.\n",
    "\n",
    "Bayes' theorem is closely related to conditional probability, because it allows us to calculate the probability of an event A given the occurrence of an event B by using conditional probabilities. Specifically, Bayes' theorem states that:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where P(A|B) is the probability of event A given the occurrence of event B, P(B|A) is the probability of event B given the occurrence of event A, P(A) is the prior probability of event A, and P(B) is the prior probability of event B.\n",
    "\n",
    "Thus, Bayes' theorem provides a way to update our beliefs about the probability of an event A given the occurrence of an event B, based on the conditional probabilities of A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a95eb0-ae97-4f65-9d8c-7ca686b138e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8aeb15-235e-4c99-a8a7-0b4f5459773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of which type of Naive Bayes classifier to use for a given problem depends on several factors, including the nature of the data, the size of the dataset, and the specific goals of the classification task. Here are some general guidelines for choosing the appropriate type of Naive Bayes classifier:\n",
    "\n",
    "If the features in the dataset are continuous variables, such as measurements of length or weight, then the Gaussian Naive Bayes classifier is often a good choice. This is because the Gaussian Naive Bayes classifier assumes that the features are normally distributed, and can estimate the mean and variance of each feature for each class.\n",
    "\n",
    "If the features in the dataset are discrete variables, such as categories or counts, then the Multinomial Naive Bayes classifier is often a good choice. This is because the Multinomial Naive Bayes classifier assumes that the features are drawn from a multinomial distribution, and can estimate the probability of each feature for each class.\n",
    "\n",
    "If the dataset has binary features, such as yes/no or true/false values, then the Bernoulli Naive Bayes classifier is often a good choice. This is because the Bernoulli Naive Bayes classifier assumes that the features are binary variables, and can estimate the probability of each feature being present or absent for each class.\n",
    "\n",
    "If the dataset has a combination of continuous and discrete features, or if the features are correlated, then the Gaussian Naive Bayes classifier may not be the best choice. In this case, a more sophisticated model such as a decision tree or random forest may be more appropriate.\n",
    "\n",
    "If the dataset is very large and the number of features is very high, then the Multinomial Naive Bayes classifier may be too computationally expensive to train. In this case, a variant of the Naive Bayes classifier such as the Complement Naive Bayes or the Hashing Vectorizer can be used to improve efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
